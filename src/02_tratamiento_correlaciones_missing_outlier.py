# -*- coding: utf-8 -*-
"""02_Tratamiento_correlaciones_missing_outlier

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-TxRgU3TJSIZ1_Aj3bpOvJEEc2jbRKMG

# **Valores missing, outlier y correlaciones**

En este notebook se realiza el estudio y preprocesamiento de las variables numéricas y categoricas. Se realizarán los siguientes pasos:

1. Cambio de tipos de variables
2. Separación en train y test
3. Análisis de cada variable con gráficos descriptivos
4. Para variables numericas: correlaciones de pearnson, estudio de outliers y estudio de valores missing
5. Para variables categoricas: relleno de valores missing, estudio de correlaciones con vCramer

## **Importo librerías**
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import plotly.express as px
import seaborn as sns
from matplotlib import pyplot as plt
from scipy.stats import chi2_contingency
import scipy.stats as ss
import warnings

pd.set_option('display.max_columns', 500)
pd.set_option('display.max_rows', 5000)

"""## **Funciones**

Voy guardando las funciones que están automatizadas y pienso que me van a servir en otros proyectos en un funciones_auxiliares.py y lo importo:

# **IMPROTARLAS AQUI NO EN CADA LINEA DE CODIGO**
"""

def plot_continuous_variable(df, col_name, target=None):
    """
    Graficar variables continuas, con y sin la variable objetivo.

    Parámetros:
    - df: pd.DataFrame
        DataFrame que contiene los datos.
    - col_name: str
        Nombre de la columna a visualizar.
    - target: str, opcional
        Variable objetivo para facetas (por defecto es None).
    """
    count_null = df[col_name].isnull().sum()  # Contar los valores nulos en la columna.

    fig, axes = plt.subplots(1, 2 if target else 1, figsize=(14, 5), dpi=90)  # Crear subgráficos

    # Graficar un histograma con KDE (Kernel Density Estimation) para la variable continua
    sns.histplot(df[col_name].dropna(), kde=True, ax=axes[0] if target else axes, color="skyblue")
    axes[0 if target else 0].set_title(f"{col_name} (nulos: {count_null})")
    axes[0 if target else 0].set_xlabel(col_name)
    axes[0 if target else 0].set_ylabel("Count")

    if target:  # Si se pasa una variable objetivo, graficamos un boxplot
        sns.boxplot(x=target, y=col_name, data=df, ax=axes[1], palette="Set2")
        axes[1].set_title(f"{col_name} por {target}")
        axes[1].set_ylabel(col_name)
        axes[1].set_xlabel(target)

    plt.tight_layout()  # Ajustar el espaciado
    plt.show()  # Mostrar la gráfica


def plot_categorical_variable(df, col_name, target=None):
    """
    Graficar variables categóricas, con y sin la variable objetivo.

    Parámetros:
    - df: pd.DataFrame
        DataFrame que contiene los datos.
    - col_name: str
        Nombre de la columna a visualizar.
    - target: str, opcional
        Variable objetivo para facetas (por defecto es None).
    """
    count_null = df[col_name].isnull().sum()  # Contar los valores nulos en la columna.

    # Manejar demasiadas categorías (limitar a las 10 más frecuentes)
    unique_vals = df[col_name].astype(str).value_counts()  # Contar los valores únicos
    if len(unique_vals) > 10:  # Si hay más de 10 categorías, limitamos a las 10 principales
        top_vals = unique_vals.head(10).index
        df = df[df[col_name].astype(str).isin(top_vals)]

    fig, axes = plt.subplots(1, 2 if target else 1, figsize=(14, 5), dpi=90)  # Crear subgráficos

    # Graficar un countplot para la variable categórica
    sns.countplot(
        x=df[col_name].astype(str),
        order=sorted(df[col_name].astype(str).unique()),
        ax=axes[0] if target else axes,
        color="skyblue"
    )
    axes[0 if target else 0].set_title(f"{col_name} (nulos: {count_null})")
    axes[0 if target else 0].set_xlabel(col_name)
    axes[0 if target else 0].set_ylabel("Count")
    axes[0 if target else 0].tick_params(axis='x', rotation=45)  # Rotar etiquetas del eje x

    if target:  # Si se pasa una variable objetivo, graficamos las proporciones de cada clase
        proportions = (
            df.groupby(col_name)[target]  # Agrupar por la variable categórica
            .value_counts(normalize=True)  # Calcular las proporciones de la variable objetivo
            .rename("proportion")
            .reset_index()
        )
        sns.barplot(
            x=col_name,
            y="proportion",
            hue=target,
            data=proportions,
            ax=axes[1],
            palette="Set2"
        )
        axes[1].set_title(f"Proporciones de {target} por {col_name}")
        axes[1].set_xlabel(col_name)
        axes[1].set_ylabel("Proporción")
        axes[1].tick_params(axis='x', rotation=45)

    plt.tight_layout()  # Ajustar el espaciado
    plt.show()  # Mostrar la gráfica


def plot_feature_variable(df, col_name, isContinuous, target=None):
    """
    Decidir qué función de graficado llamar según el tipo de variable (continua o categórica).

    Parámetros:
    - df: pd.DataFrame
        DataFrame que contiene los datos.
    - col_name: str
        Nombre de la columna a visualizar.
    - isContinuous: bool
        Si la variable es continua o categórica.
    - target: str, opcional
        Variable objetivo para facetas (por defecto es None).
    """
    if isContinuous:
        plot_continuous_variable(df, col_name, target=target)  # Si es continua, llamar a la función para variables continuas
    else:
        plot_categorical_variable(df, col_name, target=target)  # Si es categórica, llamar a la función para variables categóricas


def analyze_outliers(credit_procesado, list_var_num, target, multiplier=3):
    """
    Analiza los outliers para las variables numéricas y devuelve los resultados.
    - df: DataFrame
    - list_var_num: lista de las variables numéricas a analizar
    - target: variable objetivo para análisis
    - multiplier: factor para detectar los outliers (por defecto 3 veces el IQR)
    """
    outliers = []
    for col in list_var_num:
        q1 = credit_procesado[col].quantile(0.25)
        q3 = credit_procesado[col].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - multiplier * iqr
        upper_bound = q3 + multiplier * iqr

        # Identifica los outliers en la columna
        outliers_in_col = credit_procesado[(credit_procesado[col] < lower_bound) | (credit_procesado[col] > upper_bound)]
        outliers.append((col, len(outliers_in_col)))

    # Crear un DataFrame con los resultados de los outliers
    outlier_df = pd.DataFrame(outliers, columns=["Variable", "Outlier Count"])
    return outlier_df

def treat_outliers(credit_procesado, list_var_num, multiplier=3, method="median"):
    """
    Trata los outliers en las variables numéricas reemplazándolos por la media o la mediana.

    - df: DataFrame
    - list_var_num: lista de las variables numéricas a tratar
    - multiplier: factor para detectar los outliers (por defecto 3 veces el IQR)
    - method: 'median' o 'mean' para indicar qué método utilizar para reemplazar los outliers
    """
    for col in list_var_num:
        q1 = credit_procesado[col].quantile(0.25)
        q3 = credit_procesado[col].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - multiplier * iqr
        upper_bound = q3 + multiplier * iqr

        outliers_in_col = (credit_procesado[col] < lower_bound) | (credit_procesado[col] > upper_bound)

        if method == "median":
            median_value = credit_procesado[col].median()
            credit_procesado[col] = credit_procesado[col].where(~outliers_in_col, median_value)

        elif method == "mean":
            mean_value = credit_procesado[col].mean()
            credit_procesado[col] = credit_procesado[col].where(~outliers_in_col, mean_value)

    return credit_procesado


def analizar_nulos(df):
    """
    Analiza los valores nulos en un DataFrame, tanto por columnas como por filas.

    - df: DataFrame a analizar.
    """

    nulos_columnas = df.isnull().sum()
    porcentaje_nulos_columnas = (nulos_columnas / len(df) * 100).round(2)

    pd_null_columnas = pd.DataFrame({
        'nulos_columnas': nulos_columnas,
        'porcentaje (%)': porcentaje_nulos_columnas
    }).sort_values(by='nulos_columnas', ascending=False)

    nulos_filas = df.isnull().sum(axis=1)
    porcentaje_nulos_filas = (nulos_filas / df.shape[1] * 100).round(2)

    pd_null_filas = pd.DataFrame({
        'nulos_filas': nulos_filas,
        'porcentaje (%)': porcentaje_nulos_filas
    }).sort_values(by='nulos_filas', ascending=False)

    return pd_null_columnas, pd_null_filas




def calcular_nulos_por_objetivo(df, target_col, tipo_variable):
    """
    Calcula el porcentaje de valores nulos por columna, agrupado por la variable objetivo (TARGET).

    - df: DataFrame que contiene los datos.
    - target_col: Nombre de la columna objetivo (TARGET).
    - tipo_variable: 'categoricas' o 'continuas' para seleccionar el tipo de variables a analizar.
    """

    if tipo_variable == 'categoricas':
        columnas = list_var_cat
    elif tipo_variable == 'continuas':
        columnas = list_var_continuous
    else:
        raise ValueError("El tipo_variable debe ser 'categoricas' o 'continuas'.")

    columnas = [col for col in columnas if col in df.columns]

    nulos_por_objetivo = pd.DataFrame(index=columnas)

    grouped = df.groupby(target_col)
    for target_value, group in grouped:
        nulos_por_objetivo[f"Target_{int(target_value)}"] = (
            group[columnas].isnull().sum() / len(group) * 100
        ).round(2)

    nulos_por_objetivo["Total_Porcentaje (%)"] = nulos_por_objetivo.mean(axis=1).round(2)

    if "Target_1" in nulos_por_objetivo.columns and "Target_0" in nulos_por_objetivo.columns:
        nulos_por_objetivo["Diferencia_0_1 (%)"] = (
            nulos_por_objetivo["Target_1"] - nulos_por_objetivo["Target_0"]
        ).round(2)

    return nulos_por_objetivo.sort_values(by="Total_Porcentaje (%)", ascending=False)



def get_corr_matrix(dataset, metodo='pearson', size_figure=[10, 8]):

    """
    Crea una matriz de correlación para las variables numéricas del dataset y la visualiza.

    - dataset: DataFrame que contiene los datos.
    - metodo: El método de correlación, por defecto 'pearson'.
    - size_figure: Tamaño de la figura, por defecto [10, 8].
    """

    if dataset is None:
        print('Hace falta pasar argumentos a la función')
        return None
    corr = dataset.corr(method=metodo)
    for i in range(corr.shape[0]):
        corr.iloc[i, i] = 0
    plt.figure(figsize=size_figure)
    sns.heatmap(corr, center=0, square=True, linewidths=.5, cmap='viridis')
    plt.show()
    return corr

def cramers_v(confusion_matrix):

    """
    Calcula el valor de Cramér's V, una medida de la asociación entre dos variables categóricas.

    - confusion_matrix: matriz de contingencia (tabla de frecuencias) entre dos variables categóricas.
   """

    chi2 = ss.chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum()
    phi2 = chi2 / n
    r, k = confusion_matrix.shape
    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))
    rcorr = r - ((r-1)**2)/(n-1)
    kcorr = k - ((k-1)**2)/(n-1)
    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))

"""## **Lectura de datos del preprocesado incial**

Lectura de los datos y cambio de tipos de variables


"""

credit_procesado = pd.read_csv(r'..\data\Processed\archivo_procesado.csv')
credit_procesado

credit_procesado.shape

credit_procesado.columns

"""Procederemos a reestructurar las variables clasificándolas nuevamente según su tipo de datos y la información que contienen, dividiéndolas en variables categóricas y continuas, de manera similar a como lo hicimos en el análisis previo.

Para esto, vamos a calcular la cantidad de valores únicos para cada variable en el conjunto de datos. Este procedimiento nos permitirá obtener una idea general de cuáles variables podrían ser categóricas y cuáles numéricas. Una vez realizados los cálculos, hemos establecido un umbral de 50 valores únicos, ya que el conjunto de datos contiene un número considerable de filas, y consideramos que este valor es adecuado para la distinción inicial. Este umbral puede ajustarse posteriormente a medida que avanzamos en el análisis.

En términos prácticos, aquellas variables cuyo número de valores únicos sea inferior a 50 serán consideradas como variables categóricas, mientras que aquellas que superen los 50 valores únicos se clasificarán como numéricas. Esto se basa en la premisa de que, generalmente, las variables categóricas tienden a tener un número limitado de valores distintos, mientras que las variables numéricas suelen tener una mayor diversidad de valores.

Al revisar las variables que hemos clasificado inicialmente como categóricas, nos hemos percatado de que algunas de ellas, a pesar de tener menos de 50 valores únicos, en realidad son variables numéricas. Un ejemplo de esto es la variable `obs_30_cnt_social_circle`, que contiene solo 33 valores únicos, pero representa el número de observaciones del entorno social del cliente con mora observable de 30 días (DPD, por sus siglas en inglés), lo cual claramente indica que se trata de una variable numérica.

En base a este análisis, hemos decidido reconsiderar la clasificación de esta y otras variables similares, cambiándolas a numéricas, ya que su naturaleza y contexto sugieren que deben ser tratadas como tales, independientemente de la cantidad de valores únicos que tengan.
"""

dict_nunique = {col: credit_procesado[col].nunique() for col in credit_procesado.columns}
filtrado_dict = {key: value for key, value in dict_nunique.items() if value < 50}

list_var_cat = list(filtrado_dict.keys())
list_var_continuous = [col for col in credit_procesado.select_dtypes(include='number').columns if col not in list_var_cat]

manual_numeric_vars = [
    'amt_req_credit_bureau_hour',
    'amt_req_credit_bureau_day',
    'amt_req_credit_bureau_week',
    'amt_req_credit_bureau_mon',
    'amt_req_credit_bureau_qrt',
    'amt_req_credit_bureau_year',
    'obs_30_cnt_social_circle',
    'def_30_cnt_social_circle',
    'obs_60_cnt_social_circle',
    'def_60_cnt_social_circle',
    'elevators_mode',
    'entrances_mode',
    'floorsmax_mode',
    'floorsmin_mode',
    'elevators_medi',
    'entrances_medi',
    'floorsmax_medi',
    'floorsmin_medi',
    'hour_appr_process_start',
    'cnt_fam_members',
    'cnt_children',
    'nonlivingapartments_mode',
    'own_car_age',

]

list_var_cat = [col for col in list_var_cat if col not in manual_numeric_vars]
list_var_continuous += manual_numeric_vars


print("Variables categóricas:", list_var_cat)
print("Variables numéricas:", list_var_continuous)

credit_procesado[list_var_cat] = credit_procesado[list_var_cat].astype("category")
credit_procesado[list_var_continuous] = credit_procesado[list_var_continuous].astype(float)
credit_procesado.dtypes

"""## **Separación en train y test estratificado**

Antes de proceder con la separación en conjuntos de entrenamiento y prueba, vamos a visualizar un gráfico de barras que muestre la distribución porcentual y el conteo de la variable `target` en los datos. Para ello, primero calcularemos el porcentaje y el conteo de cada valor en la variable `target`, los combinaremos en un solo DataFrame y luego utilizaremos Plotly para generar un histograma que visualice estos valores de manera clara.
"""

pd_plot_target = credit_procesado['target'].value_counts(normalize=True).mul(100).rename('percent').reset_index()
pd_plot_target.rename(columns={'index': 'target'}, inplace=True)

pd_plot_target_conteo = credit_procesado['target'].value_counts().rename('count').reset_index()
pd_plot_target_conteo.rename(columns={'index': 'target'}, inplace=True)

pd_plot_target_pc = pd.merge(pd_plot_target, pd_plot_target_conteo, on=['target'], how='inner')

fig = px.histogram(pd_plot_target_pc, x="target", y="percent")

fig.update_layout(
    bargap=0.2)

fig.show()

"""A continuación, vamos a preparar los datos para entrenar el modelo, dividiéndolos en dos conjuntos: uno de entrenamiento (80%) y uno de prueba (20%). La división se hace de forma estratificada, asegurando que la distribución de la variable objetivo (`target`) sea similar en ambos conjuntos. Esto es importante para que el modelo entrene y se evalúe con representaciones equilibradas de las clases. Finalmente, se combinan las características con la variable objetivo en ambos conjuntos y se imprime la distribución de las clases en cada uno para verificar la correcta representación."""

X = credit_procesado.drop('target', axis=1)
y = credit_procesado['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)

credit_procesado_train = pd.concat([X_train, y_train], axis=1)
credit_procesado_test = pd.concat([X_test, y_test], axis=1)

print('== Distribución en el conjunto de entrenamiento ==')
print(credit_procesado_train['target'].value_counts(normalize=True))

print('== Distribución en el conjunto de prueba ==')
print(credit_procesado_test['target'].value_counts(normalize=True))

"""## **Visualización descriptiva de los datos**

Visualizamos el número de valores nulos por filas y por columnas
"""

pd_null_columnas, pd_null_filas = analizar_nulos(credit_procesado_train)

print(pd_null_columnas['nulos_columnas'].shape, pd_null_filas['nulos_filas'].shape)

pd_null_columnas

pd_null_filas.head()

"""## **Distribución del resto de variables**

"""

warnings.filterwarnings('ignore')

for i in list(credit_procesado_train.columns):
    if (credit_procesado_train[i].dtype == float) & (i != 'target'):
        plot_feature_variable(credit_procesado_train, col_name=i, isContinuous=True, target='target')
    elif i != 'target':
        plot_feature_variable(credit_procesado_train, col_name=i, isContinuous=False, target='target')

"""Este código está diseñado para visualizar variables dentro de un conjunto de datos, tanto continuas como categóricas, y nos permite explorar la relación de estas variables con una variable objetivo `target`.

El código se ejecuta iterando sobre todas las columnas del conjunto de datos. Si la columna es continua y no es la variable objetivo `target`, se llama a la función plot_feature con isContinuous=True. Si la columna es categórica (o si no es continua), se llama a la misma función con isContinuous=False.

Este código nos es útil para la exploración y visualización de los datos, permitiendonos hacer un análisis preliminar de las variables continuas y categóricas.

Los gráficos generados nos ayudan a comprender la distribución de los datos y la relación de cada variable con la variable objetivo, por ejemplo,
la primera Gráfica: `SK_ID_CURR` , nos muestra la distribución de la variable `SK_ID_CURR`, que parece ser un identificador único de cada cliente, junto con un gráfico de caja (boxplot) que compara la distribución de esta variable en relación con la variable objetivo `target`.

En conclusion, este codigo nos ha permitido visualizar la distribución de una variable de identificación, como `SK_ID_CURR`, para asegurarnos de que no haya errores evidentes en los datos (como valores faltantes o atípicos), y a parte, confirmar que la variable identificadora no está relacionada con el comportamiento de la variable objetivo `target`, lo que indica que no es relevante para el análisis predictivo.

## **Tratamiento de las variables continuas**

A continuación, se tratan los valores missing, las correlaciones de las vairbales continuas y los outlier
"""

list_var_continuous

"""### **Tratamiento de outliers**"""

outlier_results = analyze_outliers(credit_procesado_train, list_var_continuous, target='target', multiplier=3)
print(outlier_results)

outlier_results.to_csv("outlier_analysis_results.csv", index=False)


for var in list_var_continuous:
    plt.figure(figsize=(8, 5))
    sns.histplot(credit_procesado_train[var], kde=True, color='skyblue')
    plt.title(f'Distribución de {var}')
    plt.xlabel(var)
    plt.ylabel('Frecuencia')
    plt.show()

# Ahora podemos analizar visualmente las distribuciones de las variables continuas
# y los outliers identificados para decidir si los eliminamos o los tratamos de alguna otra manera.

df_treated_median = treat_outliers(credit_procesado_train.copy(), list_var_continuous, method="median")
df_treated_mean = treat_outliers(credit_procesado_train.copy(), list_var_continuous, method="mean")

print("Datos tratados con la mediana:")
print(df_treated_median.head())

print("Datos tratados con la media:")
print(df_treated_mean.head())

"""Nuestro analisis comienza con la **identificación de los outliers** a través de un análisis estadístico utilizando el rango intercuartílico (IQR). Luego, para visualizar estos outliers y obtener una mejor comprensión de su distribución, se utilizan **gráficos de distribución**. Finalmente, una vez identificados y visualizados, se lleva a cabo un **tratamiento de outliers**, donde los valores atípicos se reemplazan por la **mediana** o la **media** según el método elegido, lo que permite una mejor calidad de los datos antes de alimentar el modelo predictivo. Este enfoque secuencial y detallado asegura que los outliers sean manejados adecuadamente para optimizar el rendimiento del modelo sin perder información valiosa.

 Tras este análisis hemos observado lo siguiente:
 1. Las Variables sin outliers (Outlier Count = 0):
SK_ID_CURR, REGION_POPULATION_RELATIVE, DAYS_BIRTH, HOUR_APPR_PROCESS_START, EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3, entre otras.

  Estas variables parecen tener distribuciones más controladas, probablemente con límites bien definidos o sin valores extremos significativos. Esto podría deberse a que estas características se mantienen dentro de un rango razonable debido a la naturaleza de los datos (por ejemplo, escalas limitadas como porcentajes o rangos temporales).


2. Variables con outliers moderados (Outlier Count entre 100 y 4000):
CNT_CHILDREN, AMT_CREDIT, AMT_ANNUITY, CNT_FAM_MEMBERS, OWN_CAR_AGE, entre otras.

  Estas variables muestran cierta dispersión en los datos, probablemente reflejando casos inusuales pero plausibles.

  Por ejemplo:En CNT_CHILDREN, los valores extremos podrían representar familias con un número significativamente alto de hijos.

  OWN_CAR_AGE podría tener outliers relacionados con registros de autos muy viejos o errores en los datos


 3. Posteriormente, disponemos de las Variables con muchos outliers (Outlier Count > 10,000): DAYS_EMPLOYED, AMT_REQ_CREDIT_BUREAU_MON, AMT_REQ_CREDIT_BUREAU_QRT, etc.

  Estas variables presentan una cantidad considerable de valores extremos. Algunos posibles motivos incluyen:

  DAYS_EMPLOYED: Puede contener registros inválidos (como valores extremadamente altos para personas que nunca trabajaron).

  AMT_REQ_CREDIT_BUREAU_*: Podría estar influida por errores en el registro de consultas o casos atípicos de personas con alta actividad crediticia.

### **Correlaciones**
"""

# Matriz de correlación para variables numéricas
num_corr_matrix = get_corr_matrix(dataset=credit_procesado_train[list_var_continuous], metodo='pearson', size_figure=[10, 8])

"""### **Tratamiento de valores nulos**

¿Son todos los nulos de una clase de la variable objetivo? o tienen el mismo porcentaje de la variable objetivo?

En este paso, vamos a calcular el porcentaje de valores numéricos presentes en cada variable numérica. Además, vamos a analizar la distribución de los valores nulos en cada uno de los valores únicos de la variable objetivo (TARGET). Este análisis es crucial, ya que nos permitirá identificar si existe una diferencia significativa en la proporción de valores nulos entre las dos clases de la variable objetivo (0 y 1).

Si encontramos que una columna muestra una diferencia significativa en la proporción de nulos entre las dos clases de TARGET, esto podría sugerir que la columna es relevante para el análisis. Es posible que los valores nulos en esa columna estén asociados a una clase específica de TARGET. Por ejemplo, si los nulos se concentran más en los casos donde TARGET = 1, esto podría indicar un patrón que sea útil para la predicción del objetivo.

Por otro lado, si no encontramos una diferencia significativa, podemos concluir que los valores nulos en esa columna no tienen un impacto considerable en la predicción de TARGET. En este caso, podríamos decidir no darle mucha importancia a estos nulos o, si la proporción de nulos es demasiado alta, incluso optar por eliminar la columna del análisis.

Además de los valores nulos, calcularemos la relación entre las variables numéricas y la variable TARGET. Esto nos ayudará a identificar qué variables tienen una correlación o una relación significativa con el objetivo, lo que es clave para decidir cómo tratarlas. Si una variable muestra una relación fuerte con TARGET, podría ser relevante para el modelo, y deberíamos tener precaución al manejar los valores nulos o cualquier otro tipo de datos faltantes en ella.


Esta evaluación nos permitirá tomar decisiones informadas sobre cómo manejar las columnas con valores nulos: si debemos eliminarlas, imputarlas con valores adecuados o tratarlas de otro modo según su impacto en el análisis.
"""

calcular_nulos_por_objetivo(credit_procesado_train, 'target', 'continuas')

corr_with_target = credit_procesado_train[list_var_continuous].corrwith(credit_procesado_train['target'], method='pearson')
print(corr_with_target)

#Metrica para tratar los nulos

umbral_baja_correlacion = 0.1
umbral_alta_correlacion = 0.5
umbral_nulos = 0.7


porcentaje_nulos = credit_procesado_train.isnull().mean()


for col in list_var_continuous:
    # Condición 1: Si la columna tiene una baja correlación y muchos nulos
    if (corr_with_target[col] < umbral_baja_correlacion and porcentaje_nulos[col] > umbral_nulos):
        credit_procesado_train[col] = credit_procesado_train[col].fillna(credit_procesado_train[col].median())

    # Condición 2: Si la columna tiene una alta correlación con el objetivo
    elif (corr_with_target[col] > umbral_alta_correlacion):
        credit_procesado_train[col] = credit_procesado_train[col].fillna(credit_procesado_train[col].median())

    # Condición 3: Si la columna tiene nulos distribuidos de manera desigual entre las clases del objetivo
    elif porcentaje_nulos[col] > 0:
        for target_class in credit_procesado_train['target'].unique():
            mediana_clase = credit_procesado_train[credit_procesado_train['target'] == target_class][col].median()
            credit_procesado_train.loc[(credit_procesado_train['target'] == target_class) & (credit_procesado_train[col].isnull()), col] = mediana_clase

# Eliminar columnas con más del 70% de nulos y baja correlación
columnas_a_eliminar = [col for col in list_var_continuous if porcentaje_nulos[col] > umbral_nulos and corr_with_target[col] < umbral_baja_correlacion]
credit_procesado_train = credit_procesado_train.drop(columns=columnas_a_eliminar)

"""Las métricas que hemos utilizado para tratar estos nulos:



1.   Manejo de nulos con la mediana

*  Baja correlación y altos nulos: Imputamos la mediana solo a las columnas con baja correlación con la variable objetivo y un alto porcentaje de nulos.
*   Alta correlación: Si la correlación con la variable objetivo es alta, también imputamos la mediana.

* Distribución desigual de nulos: Si los nulos están distribuidos de manera desigual según las clases de la variable objetivo `target`, imputamos la mediana por clase.


2. Eliminación de columnas:

* Si una columna tiene más del 70% de nulos y baja correlación con la variable objetivo, se elimina del DataFrame.

"""

# Imputación de nulos por la mediana de las columnas del conjunto de entrenamiento

medianas_train = credit_procesado_train[list_var_continuous].median()
credit_procesado_test[list_var_continuous] = credit_procesado_test[list_var_continuous].fillna(medianas_train)

num_corr_matrix_sin_nulos = get_corr_matrix(dataset=credit_procesado_train[list_var_continuous], metodo='pearson', size_figure=[10, 8])

"""## **Tratamiento de las variables categoricas**

### **Correlaciones**
"""

warnings.filterwarnings('ignore')

correlation_matrix = pd.DataFrame(index=list_var_cat, columns=list_var_cat)

for var1 in list_var_cat:
    for var2 in list_var_cat:
        if var1 != var2:
            confusion_matrix = pd.crosstab(credit_procesado_train[var1], credit_procesado_train[var2])
            if confusion_matrix.shape[0] > 1 and confusion_matrix.shape[1] > 1:
                cramers_v_value = cramers_v(confusion_matrix.values)
                correlation_matrix.loc[var1, var2] = cramers_v_value
            else:
                correlation_matrix.loc[var1, var2] = np.nan
        else:
            correlation_matrix.loc[var1, var2] = 1.0

correlation_matrix = correlation_matrix.apply(pd.to_numeric, errors='coerce')

plt.figure(figsize=(12, 8))

sns.heatmap(correlation_matrix, annot=False, cmap="viridis", vmin=0, vmax=1, cbar=True, linewidths=0.5, linecolor='gray')

plt.title('Matriz de Correlación de Cramér\'s V', fontsize=16)
plt.xticks(rotation=90, ha='right', fontsize=10)
plt.yticks(rotation=0, fontsize=10)

plt.tight_layout()
plt.show()

"""### **Tratamiento de valores nulos**"""

calcular_nulos_por_objetivo(credit_procesado_train, 'target', 'categoricas')

correlation_with_target = correlation_matrix['target'].sort_values(ascending=False)
correlation_with_target

boolean_columns = [
    col for col in list_var_cat
    if set(credit_procesado_train[col].dropna().unique()).issubset({0, 1})
]

non_boolean_columns = [col for col in list_var_cat if col not in boolean_columns]

for col in non_boolean_columns:
    if credit_procesado_train[col].dtype.name == 'category':
        credit_procesado_train[col] = credit_procesado_train[col].cat.add_categories('No especificado')

credit_procesado_train[non_boolean_columns] = credit_procesado_train[non_boolean_columns].fillna("No especificado")

"""En este caso, hemos decidido sustituir todos los valores nulos por "No Especificado" debido a que la proporción de nulos en las variables es baja y no afectará significativamente la distribución general de los datos. Además, muchas de estas variables son relevantes para la predicción del objetivo `target`, por lo que mantener los registros con esta categoría garantiza que no se pierda información valiosa."""

#Imputación de valores nulos (NaN) en las columnas categóricas del conjunto de datos de prueba (data_test).

boolean_columns = [
    col for col in list_var_cat
    if set(credit_procesado_test[col].dropna().unique()).issubset({0, 1})
]

non_boolean_columns = [col for col in list_var_cat if col not in boolean_columns]

for col in non_boolean_columns:
    if credit_procesado_test[col].dtype.name == 'category':
        credit_procesado_test[col] = credit_procesado_test[col].cat.add_categories('No especificado')

credit_procesado_test[non_boolean_columns] = credit_procesado_test[non_boolean_columns].fillna("No especificado")

credit_procesado_train.to_csv('credit_tratado_train.csv', index=False)

credit_procesado_test.to_csv('credit_tratado_test.csv', index=False)