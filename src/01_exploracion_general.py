# -*- coding: utf-8 -*-
"""01_Exploracion_general.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18nDMccPBXwNqy01o1OCawyYJVf60yqhh

# **Data Préstamos Bancarios**

El análisis de datos de préstamos es una tarea fundamental para los bancos y otras instituciones financieras. Utilizando datos históricos de solicitudes de préstamos, se pueden identificar patrones que ayuden a predecir la probabilidad de incumplimiento de un préstamo. Este tipo de análisis permite a las instituciones financieras tomar decisiones informadas sobre a quién otorgar un préstamo, basándose en la capacidad de pago y en el historial de comportamiento financiero de los solicitantes.

En esta práctica, trabajaremos con un conjunto de datos que contiene información de clientes que han solicitado préstamos en un banco. La tarea principal será aplicar técnicas de Análisis Exploratorio de Datos (EDA) para comprender los patrones y características de los clientes que pueden estar relacionados con el incumplimiento del pago de un préstamo. A través de este proceso, se buscará identificar señales tempranas que puedan indicar si un solicitante es probable que no pueda devolver el préstamo.

## Definición del problema:

El objetivo es identificar los factores que contribuyen a la probabilidad de que un cliente no devuelva el préstamo que ha solicitado. A través de un análisis exploratorio, se explorarán variables clave del conjunto de datos para encontrar posibles correlaciones entre las características del cliente y el riesgo de incumplimiento. Las decisiones derivadas de este análisis pueden ser utilizadas para mejorar el proceso de aprobación de préstamos, minimizando el riesgo de pérdida para el banco.

El análisis permitirá responder a la pregunta: ¿Qué tipo de clientes son más propensos a no devolver un préstamo? Esta información será clave para la formulación de estrategias más precisas en la concesión de préstamos y la gestión de riesgos financieros.

Los pasos a realizar son:

1. Análisis inicial de los datos y preprocesamiento inicial
2. Correlaciones, tratamiento de missing y outliers
4. Tratamiento de variables categoricas: encoding
5. Aplicación de algoritmos
6. Evaluación con la muestra de test

### **Importamos librerías**
"""

import os
import pandas as pd
import plotly.express as px

pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)

# Ruta relativa al archivo
ruta_csv = os.path.join('data', 'Raw', 'application_data.csv')

credit = pd.read_csv(ruta_csv)

"""### **Variables futuras**

Son variables que contienen información que no estaría disponible en el momento de la toma de decisiones, como cuando se evalúa la solicitud de un préstamo. Por ejemplo, datos que se recopilan o actualizan después de que la decisión inicial ha sido tomada. Si las incluyésemos en el modelo se crearía un sesgo de información, ya que el modelo tendría acceso a datos que no serían conocidos en una situación real.

Para seleccionar las variables futuras nos hemos basado en una evaluación general del contexto y los nombres de las columnas que parecen indicar datos recopilados o calculados después de un evento inicial, en este caso una solicitud de crédito o préstamo. Esto no es 100% seguro porque depende del contexto específico del problema y del significado exacto de cada variable en nuestro conjunto de datos.

Al analizar todas las variables, podemos decir que ninguna de ellas proporciona información sobre eventos que suceden después del momento de predicción, lo cual es lo que calificaría a una variable como futura.

`SK_ID_CURR` es un idenificador único de un préstamo ya existente. No tiene información futura, solo un registro presente o pasado.

`TARGET` es la variable objetivo.

`NAME_CONTRACT_TYPE`:Identifica el tipo de contrato del préstamo, que es un hecho definido al inicio del contrato, no algo futuro.

Los datos de perfil como `DAYS_BIRTH`, `CODE_GENDER`, `CNT_CHILDREN`, `AMT_INCOME_TOTAL`, `FLAG_OWN_CAR`, `FLAG_OWN_REALTY`, `NAME_INCOME_TYPE`, `NAME_EDUCATION_TYPE`, `NAME_FAMILY_STATUS`, `NAME_HOUSING_TYPE`, etc. son atributos fijos o cambian con muy poca frecuencia y no dependen de eventos futuros. Reflejan la situación del cliente en el momento de la evaluación y, por tanto, son seguros para incluir en el modelo.

Los valores AMT son valores calculados al momento de la solicitud:
Estas cantidades se determinan en función del monto solicitado `AMT_CREDIT`, las condiciones del contrato `AMT_ANNUITY`, o el precio del bien financiado `AMT_GOODS_PRICE`. Estos valores están definidos y fijos desde el momento en que se evalúa al cliente.

Otro datos como `FLAG_EMP_PHONE`, `FLAG_WORK_PHONE`, `FLAG_CONT_MOBILE`, `FLAG_PHONE`, `FLAG_EMAIL`, `FLAG_MOBIL`, entre otros, son datos actuales, basados en la información proporcionada por el cliente.

`REGION_RATING_CLIENT` y `REGION_RATING_CLIENT_W_CITY` son datos basados en información geográfica actual.

Las consultas al Buró de Crédito `AMT_REQ_CREDIT_BUREAU_` muestran la cantidad de veces que la institución ha consultado el historial de crédito del cliente en distintos períodos (días, semanas, meses, trimestres, años).

En realidad, cada consulta puede considerarse información obtenida antes o en el momento de solicitar el préstamo, como parte del historial. Si están disponibles en el momento de evaluar el riesgo, no son futuras. Solo reflejan el comportamiento previo del cliente en términos de acceso a crédito, no el resultado de pago.

Finalmente hemos concluido que todas estas variables reflejan características y comportamientos previos o presentes del cliente, no eventos posteriores a la predicción. Esto significa que no están sesgando el mode`o con información de un período que suceda después del que queremos analizar.

Si hubiera habido alguna variable futura, el procedimiento a seguir habría sido el siguiente:


```python
# Variables futuras identificadas
list_future_variables = [
    'NOMBRE_VARIABLE_FUTURA_1',
    'NOMBRE_VARIABLE_FUTURA_2',
    'NOMBRE_VARIABLE_FUTURA_3',
    'NOMBRE_VARIABLE_FUTURA_4',
    'NOMBRE_VARIABLE_FUTURA_X'
]

# Eliminar variables futuras y otras variables irrelevantes
data_cleaned = credit.drop(columns = list_future_variables)

# Confirmar que las variables han sido eliminadas
print("Columnas restantes después de eliminar variables futuras:")
print(data_cleaned.columns)
```
Y de aquí en adelante trabajaríamos con el nuevo dataset sin variables futuras: data_cleaned.

Como no hay variables futuras, continuamos trabajando con el dataset original "credit".

### **Análisis generales de la tabla**
Dimensiones
"""

print(credit.shape, credit.drop_duplicates().shape)

credit

"""Tipos de datos"""

credit.dtypes.to_dict()

"""### **Exploración de la variable objetivo y tratamiento**"""

pd_plot_target = credit['TARGET'].value_counts(normalize=True).mul(100).rename('percent').reset_index()
pd_plot_target.rename(columns={'index': 'TARGET'}, inplace=True)

pd_plot_target_conteo = credit['TARGET'].value_counts().rename('count').reset_index()
pd_plot_target_conteo.rename(columns={'index': 'TARGET'}, inplace=True)

pd_plot_target_pc = pd.merge(pd_plot_target, pd_plot_target_conteo, on='TARGET', how='inner')

print(pd_plot_target_pc)

fig = px.bar(pd_plot_target_pc, x='TARGET', y='percent', text='count')

fig.update_layout(
    title='Distribución de la variable objetivo',
    xaxis_title='TARGET',
    yaxis_title='Porcentaje (%)',
    template='plotly_white'
)

fig.show()

"""El eje X indica los valores posibles de la variable TARGET, que en este caso son 0 (clientes SIN dificultades de pago) y 1 (clientes CON dificultades de pago).

Esto quiere decir que:
- si `TARGET` = 0: Los clientes cumplieron con sus pagos.
- si `TARGET` = 1: Los clientes tuvieron impagos o problemas financieros.

El eje Y muestra el porcentaje de observaciones para cada clase.

La clase 0 (sin dificultades de pago) tiene una mayor proporción, representando aproximadamente 90% del total.
La clase 1 (con dificultades de pago) es significativamente menor, aproximadamente 10% del total.

La gráfica muestra un problema de clases desbalanceadas, común en datasets financieros donde la mayoría de los clientes no tienen problemas de pago (0), mientras que los casos de incumplimiento (1) son una minoría.

### **Selección de threshold por filas y columnas para eliminar valores missing**
"""

# Calculamos valores faltantes por columna y por fila
pd_series_null_columns = credit.isnull().sum().sort_values(ascending=False)
pd_series_null_rows = credit.isnull().sum(axis=1).sort_values(ascending=False)
print(pd_series_null_columns.shape, pd_series_null_rows.shape)

# Creamos DataFrames para almacenar información sobre valores nulos
pd_null_columnas = pd.DataFrame(pd_series_null_columns, columns=['nulos_columnas'])
pd_null_filas = pd.DataFrame(pd_series_null_rows, columns=['nulos_filas'])

# Añadimos columnas con porcentaje de nulos
pd_null_columnas['porcentaje_columnas'] = pd_null_columnas['nulos_columnas'] / credit.shape[0]
pd_null_columnas = pd_null_columnas.sort_values(by='porcentaje_columnas', ascending=False)
pd_null_filas['porcentaje_filas'] = pd_null_filas['nulos_filas'] / credit.shape[1]
pd_null_filas = pd_null_filas.sort_values(by='porcentaje_filas', ascending=False)

# Mostramos estadísticas iniciales
print(f"Dimensiones iniciales del dataset: {credit.shape}")

print(pd_null_columnas.head())
print(pd_null_filas.head())

"""Este código muestra de mayor a menor el % de nulos en cada fila y en cada columna. Como el mayor %, tanto en filas como en columnas, no alcanza el 90%, no tenemos que eliminar nada.

En caso de que hubiese habido alguno mayor que 90%, el procedimiento a seguir habría sido el siguiente:

```python
# Definimos el threshold para eliminar columnas y filas
threshold_columnas = 0.9
threshold_filas = 0.9

# Filtramos columnas que tienen menos del 90% de valores faltantes
list_vars_not_null = list(pd_null_columnas[pd_null_columnas['porcentaje_columnas'] < threshold_columnas].index)
data_filtered_columns = data.loc[:, list_vars_not_null]
print(f"Dimensiones después de filtrar columnas con más del {threshold_columnas*100}% de nulos: {data_filtered_columns.shape}")

# Filtramos filas que tienen menos del 90% de valores faltantes
data_filtered = data_filtered_columns[data_filtered_columns.isnull().sum(axis=1) / data_filtered_columns.shape[1] < threshold_filas]
print(f"Dimensiones después de filtrar filas con más del {threshold_filas*100}% de nulos: {data_filtered.shape}")
```

### **Tipos: Variables categoricas y numericas**

Vamos a calcular la cantidad de valores únicos para cada variable en el conjunto de datos. Este procedimiento nos permitirá obtener una idea general de qué variables podrían ser categóricas y cuáles numéricas. Una vez realizados los cálculos, hemos establecido un umbral de 50 valores únicos, ya que el conjunto de datos contiene un número considerable de filas, y consideramos que este valor es adecuado para la distinción inicial. Este umbral puede ajustarse posteriormente a medida que avanzamos en el análisis.

En términos prácticos, aquellas variables cuyo número de valores únicos sea inferior a 50 serán consideradas como variables categóricas, mientras que aquellas que superen los 50 valores únicos se clasificarán como numéricas. Esto se basa en la premisa de que, generalmente, las variables categóricas tienden a tener un número limitado de valores distintos, mientras que las variables numéricas suelen tener una mayor diversidad de valores.

Al revisar las variables que hemos clasificado inicialmente como categóricas, nos hemos percatado de que algunas de ellas, a pesar de tener menos de 50 valores únicos, en realidad son variables numéricas. Un ejemplo de esto es la variable `OBS_30_CNT_SOCIAL_CIRCLE`, que contiene solo 33 valores únicos, pero representa el número de observaciones del entorno social del cliente con mora observable de 30 días (DPD, por sus siglas en inglés), lo cual claramente indica que se trata de una variable numérica.

Con base en este análisis, hemos decidido reconsiderar la clasificación de esta y otras variables similares, cambiándolas a numéricas, ya que su naturaleza y contexto sugieren que deben ser tratadas como tales, independientemente de la cantidad de valores únicos que tengan.
"""

dict_nunique = {col: credit[col].nunique() for col in credit.columns}
filtrado_dict = {key: value for key, value in dict_nunique.items() if value < 50}

list_var_cat = list(filtrado_dict.keys())
list_var_continuous = [col for col in credit.select_dtypes(include='number').columns if col not in list_var_cat]

manual_numeric_vars = [
    'AMT_REQ_CREDIT_BUREAU_HOUR',
    'AMT_REQ_CREDIT_BUREAU_DAY',
    'AMT_REQ_CREDIT_BUREAU_WEEK',
    'AMT_REQ_CREDIT_BUREAU_MON',
    'AMT_REQ_CREDIT_BUREAU_QRT',
    'AMT_REQ_CREDIT_BUREAU_YEAR',
    'OBS_30_CNT_SOCIAL_CIRCLE',
    'DEF_30_CNT_SOCIAL_CIRCLE',
    'OBS_60_CNT_SOCIAL_CIRCLE',
    'DEF_60_CNT_SOCIAL_CIRCLE',
    'ELEVATORS_MODE',
    'ENTRANCES_MODE',
    'FLOORSMAX_MODE',
    'FLOORSMIN_MODE',
    'ELEVATORS_MEDI',
    'ENTRANCES_MEDI',
    'FLOORSMAX_MEDI',
    'FLOORSMIN_MEDI',
    'HOUR_APPR_PROCESS_START',
    'CNT_FAM_MEMBERS',
    'CNT_CHILDREN'
]

list_var_cat = [col for col in list_var_cat if col not in manual_numeric_vars]
list_var_continuous += manual_numeric_vars


print("Variables categóricas:", list_var_cat)
print("Variables numéricas:", list_var_continuous)

credit[list_var_cat] = credit[list_var_cat].astype("category")
credit[list_var_continuous] = credit[list_var_continuous].astype(float)

print(credit.dtypes)

"""### **Preprocesamiento inicial de algunas variables**

En este punto, vamos a modificar algunos aspectos del DataFrame para hacerlo más limpio y legible. En primer lugar, vamos a convertir todos los nombres de las columnas a minúsculas para mantener una convención uniforme. Por otro lado, eliminaremos los espacios en blanco, si es que los hay, al principio y al final de las cadenas de texto en todas las columnas de tipo "object".

Además, vamos a transformar la variable `WEEKDAY_APPR_PROCESS_START`. En lugar de tener los días de la semana escritos como texto, los reemplazaremos por números que representen su orden (lunes = 1, martes = 2, etc.). Esto facilitará la codificación posterior, permitiendo representar los días como "Weekday_1", "Weekday_2", etc., lo que puede ser útil para análisis posteriores.

Por lo general, no hemos identificado otros aspectos que requieran limpieza en este momento, por lo que consideramos que este DataFrame ya está preprocesado y listo para el tratamiento de valores faltantes, detección de valores atípicos, cálculo de correlaciones, entre otros análisis.
"""

credit.columns = credit.columns.str.lower()

credit = credit.apply(lambda x: x.str.strip() if x.dtype == "object" else x)

weekday_mapping = {
    'MONDAY': 1, 'TUESDAY': 2, 'WEDNESDAY': 3,
    'THURSDAY': 4, 'FRIDAY': 5, 'SATURDAY': 6, 'SUNDAY': 7
}

credit['weekday_appr_process_start'] = credit['weekday_appr_process_start'].map(weekday_mapping)

credit.head()

credit.shape

# RUTA RELATIVA
credit.to_csv(r'..\data\Processed\archivo_procesado.csv', index=False)